\documentclass[a4paper,11pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
%\usepackage[italian]{babel}

%\usepackage{lipsum}
\usepackage{float}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{multirow}
\usepackage{hyphenat}
\usepackage{sectsty}
%\usepackage{subfigure}
%\usepackage{color}
%\usepackage[dvipsnames]{xcolor}
%\sectionfont{\bfseries\Large\raggedright}
\usepackage{hyperref}
\allsectionsfont{\raggedright}
\graphicspath{ {images/} }


\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%\bibliographystyle{apalike}
\usepackage[dashed=false, maxnames=1, uniquelist=false, backend=bibtex,sorting=none,style=authoryear-icomp]{biblatex}
\renewbibmacro{in:}{}
\setlength{\bibhang}{0pt}
\renewcommand*{\bibfont}{\small}
\renewcommand*{\finentrypunct}{}
\addbibresource{bibliography.bib}

%\usepackage[T1]{fontenc}

\pagestyle{headings}


\title{Detection of a transiting Hot Jupiter around WASP-44}
\author{Adriana Barbieri \and Alessandro Bianchetti}

\begin{document}
\maketitle

\begin{abstract}

\emph{In the following report we work on WASP-44 b, an exoplanet orbiting 
arounf its G-type mother star, located in the constellation of Cetus. 
We first take a look at its atmospheric parameters and derive mass and 
radius (\cite{Morton}). Then, we correct for limb darkening effect in different ways 
(\cite{claret2011}, \cite{claret2017}, \cite{claret2018}). We also take 
some images taken by Copernico at the Asiago Observatory into consideration 
and, after proper correction, we use them to extract the light curve of 
the alleged planet.}

\end{abstract}

\section{Introduction}

Confirmed exoplanets are growing in number year by year, and transit method 
is nowadays a widely spread method. Most planets nowadays are discovered 
by tracing the lightcurve and searching for any sign of a weakening in the 
flux.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.15, angle=0]{../pictures/exo_dischist.png}
    \caption*{\textit{ https://exoplanetarchive.ipac.caltech.edu/}}
\end{figure}
In this report we focus on WASP-44 b, a Jupyter-size planet orbiting around 
a G-type star, located in the constellation of Cetus.
Among the numerous available reports on the planet, we decided to make a 
conservative choice and avoid any result which is not inferred via 
spectroscopy. This leads us to rule out several papers about the object, and 
only work with the discovery paper (\cite{Anderson}). In this paper, estimates 
of the atmospheric parameters of WASP-44 are provided via an analysis of 
the width of spectral lines.

%\section{Instruments}




\section{Theoretical recap}

TODO
\begin{itemize}
    \item a brief overview of the transit method
    \item a comment on the bias of the transit method (big planets, close to the star)]

\end{itemize}


\section{Data analysis}

\subsection{Inferring mass and radius}

The $H_{\alpha}$ line was used to determine the effective temperature (Teff ),
while the $NaI$ D and $MgI$ b lines were used as surface gravity
($\log{g^*}$) diagnostics (\cite{Anderson}). The elemental abundances, including $[Fe/H]$ 
were determined from equivalent width measurements of several clean and 
unblended lines. This led to proper estimation of the atmospheric parameter 
triplet $T_{eff}$, $\log{g^*}$ and $[Fe/H]$. Quoted errors include statistical 
uncertainties only. In the same conservative spirit we 
previously showed, we add in quadrature a further term to the errors of 
all three parameters (\cite{Sousa}). We're in fact more interested in an 
accurate result rather than a precise one.
This leads to the following results 
\begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    $T_{eff}$ (K) & $\log{g^*}$ & $[Fe/H]$ \\
    $5400 \pm 162$ & $4.5 \pm 0.2$ & $0.06 \pm 0.11$ \\
    \hline
    \end{tabular}
\end{center}


\subsection{Limb darkening correction}

Limb darkening is an important effect when observing a star, 
everything but negligible. In short, the edges of the luminosity profile 
of a star always look darker: this is because there is a physical, constant 
distance $L$ at which optical depth is equal to unity, further than which 
we cannot observe (photons do not reach us). This characteristic size, 
however, can go deep inside the hot layers of the star if we look straight 
to the center, being $L$ radial, while it can stop at colder, outer layers 
if we look at the edges of the star, since $L$ and our LOS are not radial 
anymore.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.25, angle=0]{../pictures/limb_darkening.png}
    \caption{Limb darkening effect scheme. Credits to \textit{https://ediss.sub.uni-hamburg.de}}
\end{figure}
Correcting for this effect may be challenging. In fact we can measure it 
directly only for the Sun, while we need to model it somehow for any other 
star, meaning figure out a proper law for the intensity decrease $I(\mu)$, 
where $\mu = \sqrt{1-r^2}$. Many choices are plausible at this point: a 
uniform behaviour, a linear, a quadratic, a square-root or a 
logarithmic law are all valid guesses. Parametrizing such laws introduces 
the so-called \textit{LD-coefficients}, which will depend on the stellar 
parameters. Knowing the latter relationship (for instance calibrating it 
based on a large sample of stars) allows to obtain the coefficients directly 
from the atmopsheric parameters. 
The alternative way is fitting the light curve leaving the coefficients free.

The choice of the functional dependence on $\mu$ is a delicate one. Multiple 
approaches can be followed, basing on different papers. This analysis is 
tackled in \ref{sect:app_A}.


\subsection{Bias and flat field correction}
CCD (Charged Coupled Device) are the privileged detectors for photon counting, 
thanks to their great quantum efficiency. The images produced are \textit{raw}
and must be properly \textit{pre-reduced} before being analysed. Pre-reduction 
goes through different steps:
\begin{itemize}
    \item \textbf{bias} is the
    individual pixel-to-pixel offset level, and it's a zero-exposure instrumental 
    factor, thus it's always an added contribution to any signal. Therefore it must 
    be removed to isolate the photons of astrophysical origin. The root mean square 
    of the bias corresponds to the so-called \textit{readout noise}. We perform an 
    average out across all the pixels to get an estimate of the bias;
    \item a \textbf{flat field} is a calibration image obtained by illuminating
    homogeneously the pupil of the telescope, using twilight sky or appropriate, 
    back-lighted screens. This correction factor is to be applied on each pixel and 
    then also normalised, after the overall bias correction;
    \item \textbf{differential photometry} is a great way to keep track of any noise
    variation. The idea is to take a reference star close enoguh to the target: any 
    environmental or instrumental variation will affect both sources. Working with 
    flux ratios will make only astrophysical variations evident!
\end{itemize}
For pre-reductions steps, we use the code \textit{huggy}. We first perform bias
correction, than the flat field correction. The corrected images are ready for 
aperture synthesis.

\subsubsection{Bias correction}
We run \textit{huggy-bias.e} inputting the raw images. 
\begin{lstlisting}
huggy-bias.e A*.fits
\end{lstlisting}
Many bias images are produced,
displaying the zero offset of the pixel board. \textit{Master bias} is the average 
of all these files. Plotting the intensities of offset level of the pixels yields 
a concrete view of the correction.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.32, angle=0]{../pictures/pre-reduction/bias.png}
    \includegraphics[scale=0.32, angle=0]{../pictures/pre-reduction/master_bias.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/pre-reduction/bias_comp.png}
    \caption{Comparison between random bias frame and master bias}
\end{figure}
Note that the master bias frame distribution is peaked and much closer to a 
unique constant value, as all pixels behaved in the same way, like in an 
ideal situation. We can see this even numerically, by looking at the 
dispersions of the above distributions: $\sigma_{rb} = 3.82$ and 
$\sigma_{mb} = 0.89$.

\subsubsection{Flat field correction}
We now run \textit{huggy-flat.e} inputting a normalization fraction, 
meaning the fraction of pixels we want to account for, ruling out the 
sides which are often polluted by overscan columns. These are visible 
in the form of dark stripes on the sides of any flat field take. Then 
we have to input the overscan values, usually set to 0. After, that 
we need to provide the master bias file, which will be subtracted from 
the data. Finally, we input all the flat fields available.
\begin{lstlisting}
huggy-flat.e 0.9 0 0 mb.fits A*.fits
\end{lstlisting}
This will produce an outpute file showing the average response of
pixels to an external light source. Minor differences in such response 
are important and must be accounted for.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.32, angle=0]{../pictures/pre-reduction/flat.png}
    \includegraphics[scale=0.32, angle=0]{../pictures/pre-reduction/master_flat.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/pre-reduction/flat_comp.png}
    \caption{Comparison between random flat field and master flat}
\end{figure}

\subsection{Final correction}
The final step is applying the correction files obtained in the 
previous parts.
\begin{lstlisting}
huggy-correct.e mb.fits mfn.fits A*.fits
\end{lstlisting}
The final correction code requires the master bias and the normalised 
master flat, and of course all the target images, that are going to be 
all corrected.

\subsection{Extracting the light curve}
We are approaching the very heart of this analysis, preparing for 
aperture synthesis. We now need to display the science images, make 
sure to identify the correct target and select a proper background 
analysis around it. To do that, we have to properly circle the source 
and create a slightly larger circle around it ($ds9$ was the used tool).
The same procedure must be repeated (with the same inner and outer radii) 
on a properly selected reference star, close to and roughly as bright 
as the target. We hereby report the coordinates of the target and of the 
chosen reference star, in pixel units.
\begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
     & $x_c$ & $y_c$ \\
    \hline
    Target    &  170  &  37    \\
    Reference & 288 & 57  \\
    \hline
    \end{tabular}

    \medskip

    $R_{in} = 11$, $R_{out} = 20$
\end{center}
Now we need to call \textit{huggy-psf.e} and input the coordinates of the 
center of the target, the inner and outer radii, and a random corrected 
image.
\begin{lstlisting}
huggy-psf.e x0 y0 Rin Rout A*.fits
\end{lstlisting}
This yields information about the Point Spread Function, meaning how the 
the flux is distributed as a function of the distance from the core of 
the source. The ouput displays the radii at which we find 90, 95 and 99\%
of the total flux. We tale note of this output. 

Photometry analysis is carried out by \textit{sentinel.e}, which needs 
the coordinates of the target and of the reference stars, followed by 
the radii of two selected apertures from the previous output. Also, we need 
again inner and outer radii of the selected region for the analysis, as well 
as the full-corrected images of the target. 
\begin{lstlisting}
sentinel.e x0 y0 xr yr a1 a2 Rin Rout c A*.corr.fits
\end{lstlisting}
where $c=\{1,2\}$ selects the centroiding method (gaussian/moment). For a 
relatively faint star like ours ($J\approx 11$) oure choices are $a_1=4.71$ and 
$a_2=5.97$, corresponding to 90 and 95\%. 

\medskip

The \textit{sentinel} output is the starting point for TASTE analysis. This file 
contains the main information about the target and the sky, keeping track of the 
respective photometries. In fact, let's start by checking the evolution of the 
sky background throughout the observation session, and also to the behaviour 
of the reference star.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45, angle=0]{../pictures/taste/sky-ref.png}
\end{figure}
The sky flux has been constantly decreasing in time, so we expect less and less 
disturbance as the observation proceeds. The reference star, on the other hand 
seems to be pretty stable in terms of flux. 
We also want to make sure that reference star are correctly comoving: to do that, 
we plot the variation in position at every time step and check we have similar 
motion.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45, angle=0]{../pictures/taste/target-ref_position.png}
\end{figure}
Next, we compare the flux plot with the two different aperture we have chosen.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45, angle=0]{../pictures/taste/apertures.png}
\end{figure}
The second case (95\%) seems to be slighly less noisy: that's our first clue 
that this aperture might be the best choice for the aperture analysis.

As a further goodness check, one may take a look at the peak value trend of 
the target source and compare it to the saturation level.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45, angle=0]{../pictures/taste/saturation-quality.png}
\end{figure}
The peak value is well below the saturation limit, and we also see that the 
quality factor is above the required threshold.

We can can finally visualise the transit, with both apertures.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45, angle=0]{../pictures/taste/transits.png}
\end{figure}
We still see that the wider aperture seems to have less dispersion than 
the other case, and we keep that in mind.

In order to evaluate the depth of the transit, we need to identify the "height" 
of the continuous level: to do that, we need to exclude the transit datapoints 
and try out a polynomial fit, hoping to find a perfectly horizontal line. That will 
be our reference for the stellar flux.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45, angle=0]{../pictures/taste/fit_setting.png}
\end{figure}
To perform the fit, we ruled out all the points during the transit by roughly 
selecting an ingress and egress time. A quadratic law ($p_0 x^2 + p_1 x +p_2$)
was chosen to set things up in the most general case. A least square fitting 
via numpy function \textit{np.polyfit} yields the following results
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45, angle=0]{../pictures/taste/fit.png}
\end{figure}
\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \# ap & $p_0 (\cdot 10^{-10})$ & $p_1 (\cdot 10^{-7})$ & $p_2 ( \cdot 10^3)$ \\
    \hline
    1 &  $-7.98$   & $1.43$    &  $4.82$\\
    2 & $-8.98$ & $1.42$  & $5.43$\\
    \hline
    \end{tabular}
\end{center}
We immediately notice that both $p_0$ and $p_1$ are well compatible with zero,
meaning that only a constant term survives: that represents the level of the 
continuous, with respect to which we're going to compute the depth of the
transit.

\section{Conclusions}















\appendix
\section{Limb darkening analysis}
\label{sect:app_A}

\subsection{Claret 2017}
We can represent data tables in \cite{claret2017} as 2D histograms, 
after proper unfolding of the data tables attached to the paper. To do that, 
we first fix metallicity, then gravity and see how the corresponding LD 
coefficients $c_1$ and $c_2$ depend on all three atmospheric parameters. 
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.25, angle=0]{../pictures/Claret2017/2017_c1_fixedmet}
    \includegraphics[scale=0.25, angle=0]{../pictures/Claret2017/2017_c2_fixedmet}

    \includegraphics[scale=0.25, angle=0]{../pictures/Claret2017/2017_c1_fixedg}
    \includegraphics[scale=0.25, angle=0]{../pictures/Claret2017/2017_c2_fixedg}
    \caption{LD coefficient with fixed metallicty, fixed gravity}
\end{figure}
Furthermore, it's very interesting to check the strength of the dependance 
on each atmospheric parameters. Turns out LD coefficients are essentially a 
function of temperature, and minor dependences on gravity and metallicity 
can be barely appreciated.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.25, angle=0]{../pictures/Claret2017/2017_c1}
    \includegraphics[scale=0.25, angle=0]{../pictures/Claret2017/2017_c2}
    \caption{LD coefficient as a function of atmospheric parameters}
\end{figure}
A relevant dependance on gravity and metallicity can only be noticed at 
high temperatures, where we should carefully select the proper curve. But 
in the range we're interested in ($\approx 5400 K$) the curve is 
degenerate and the choice of these parameters is secondary.

We perform a Montecarlo simulation, generating 1000 random atmospheric 
parameters around the actual ones. Even in this case we see that the 
distribution of the fixed-metallicity estimates is almost overlapping 
with the fixed-gravity one, thus confirming $c_1$ and $c_2$ not very 
sensitive on metallicity and gravity.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2017/2017_c1_comp}
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2017/2017_c2_comp}
    \caption{Montecarlo simulation for $c_1$ and $c_2$, for fixed metallicity 
    and for fixed gravity}
\end{figure}
The two estimates are well compatible, thus authorizing a weighted average: 
$c_1 = 0.366 \pm 0.019$ and $c_2 = 0.243 \pm 0.011$.

Another way to deal with the same table is by selecting from the set two values
of metallicity or gravity, an upper and lower limit, instead of just one reference 
value. This way we build two matrices and 
interpolate between the two to get to the desired result. The rest of the procedure is 
the same as just explained, leading to other estimate of the LD coefficients.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2017/double_logg4}
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2017/double_logg45}
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2017/double_logg_interp}
    \caption{Matrices obtained by fixing gravity to $\log{g}=4.0$ to $4.5$, plus the interpolated matrix}
\end{figure}
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2017/double_c1}
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2017/double_c2}
    \caption{LD coefficients trend in the $T_{eff}-[Fe/H]$ plane}
\end{figure}
This can be done in the same exact way by fixing metallicity instead.

\subsection{Claret 2018}
A successive paper provided a new method to model the LD coefficients dependency 
on atmospheric parameters. We'd like to check if this leads to different results 
with respect to the 2017 paper. Unfolding the table requires the same procedure 
we've already described. Just note that in both cases we fix metallicity, and for 
the 2018 table metallicity must be fixed to 0, since the method is conceived for
zero-metallicity stars.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.35, angle=0]{../pictures/ClaretvClaret/2017.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/ClaretvClaret/2018.png}
    \caption{Final, interpolated matrices for the two methods (2017 left, 2018 right)}
\end{figure}
And finally we can display the dependency of the LD coefficients for the 
two-parameters game.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.35, angle=0]{../pictures/ClaretvClaret/c1.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/ClaretvClaret/c2.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/ClaretvClaret/c1_comp.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/ClaretvClaret/c2_comp.png}
    \caption{$c_1$ and $c_2$ trend, and comparison between the final results}
\end{figure}
See how there is an actual difference between the adopted methods, that lead 
in fact to slightly different results for the LD coefficients. The good news is, 
they're largely compatible: see how each value is within a few errorbars away 
from the other.

\subsection{Claret 2011}
We use another table from an older paper by the same author, where even 
filters have a role. By looking at the header of our dataset, we find the 
right filter to be $r^{*}$ (SDSS). Remember that transits look differently 
when observed through different filters, and also that boxier transits make 
ingress/egress time determination easier. The unfolding technique is always 
the same.
This time, we account for stellar models (ATLAS/PHOENIX) and interpolation 
technique (least squares/flux conservation), for a grand total of 4 
combinations. These are the very final results.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2011/c1.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2011/c2.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2011/c1_comp.png}
    \includegraphics[scale=0.35, angle=0]{../pictures/Claret2011/c2_comp.png}
    \caption{$c_1$ and $c_2$ trend, and comparison between the final results}
\end{figure}
Again, all results are fully compatible.

\section{Time correction}
When dealing with \textit{sentinel.dat} output, we need to properly convert the 
time series, contained in columns two of the datatable. To do that, first we
convert from minutes to days, then add the zero-point in Julian Date format, and 
finally also add half exposure time (after proper conversion in days).

Moreover, we can make use of the \textit{Time} method to keep count of the time 
taken by the light to travel from the source position to the location of the 
observatory (Cima Ekar, Asiago). Since the Earth is in motion, light time 
travel is variable and follows a sinusoidal behaviour.
\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.55, angle=0]{../pictures/time_corr.png}
\end{figure}


\printbibliography

\end{document}